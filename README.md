<div align="center">

![Pixelbot](https://github.com/user-attachments/assets/29c6b22c-60cf-4d5e-8e72-58c6ca746dac)

[![License](https://img.shields.io/badge/License-Apache%202.0-0530AD.svg)](https://opensource.org/licenses/Apache-2.0) [![My Discord (1306431018890166272)](https://img.shields.io/badge/ğŸ’¬-Discord-%235865F2.svg)](https://discord.gg/QPyqFYx2UN)
<br>

![Overview](/static/image/overview.gif)

</div>

[Pixelbot](http://agent.pixeltable.com/), a multimodal context-aware AI agent built using [Pixeltable](https://github.com/pixeltable/pixeltable) â€” open-source AI data infrastructure. The agent can process and reason about various data types (documents, images, videos, audio), use external tools, search its knowledge base derived from uploaded files, generate images, maintain a chat history, and leverage a selective memory bank.

The endpoint is built with Flask (Python) and the frontend with vanilla JS. This open source code replicates entirely what you can find at https://agent.pixeltable.com/ that is hosted on AWS EC2 instances.

## ğŸš€ How Pixeltable Powers This App

Pixeltable acts as AI Data Infrastructure, simplifying the development of this complex, infinite-memory multimodal agent:

-   ğŸ“œ **Declarative Workflows**: The entire agent logicâ€”from data ingestion and processing to LLM calls and tool executionâ€”is defined declaratively using Pixeltable **tables**, **views**, and **computed columns** (`setup_pixeltable.py`). Pixeltable automatically manages dependencies and execution order.
-   ğŸ”€ **Unified Data Handling**: Natively handles diverse data types (documents, images, videos, audio) within its tables, eliminating the need for separate storage solutions.
-   âš™ï¸ **Automated Processing**: **Computed columns** automatically trigger functions (like thumbnail generation, audio extraction, transcription via Whisper, image generation via DALL-E) when new data arrives or dependencies change.
-   âœ¨ **Efficient Transformations**: **Views** and **Iterators** (like `DocumentSplitter`, `FrameIterator`, `AudioSplitter`) process data on-the-fly (e.g., chunking documents, extracting video frames) without duplicating the underlying data.
-   ğŸ” **Integrated Search**: **Embedding indexes** are easily added to tables/views, enabling powerful semantic search across text, images, and frames with simple syntax (`.similarity()`).
-   ğŸ”Œ **Seamless Tool Integration**: Any Python function (`@pxt.udf`) or Pixeltable query function (`@pxt.query`) can be registered as a tool for the LLM using `pxt.tools()`. Pixeltable handles the invocation (`pxt.invoke_tools()`) based on the LLM's decision.
-   ğŸ’¾ **State Management**: Persistently stores all relevant application state (uploaded files, chat history, memory, generated images, workflow runs) within its managed tables.

```mermaid
flowchart TD
    %% User Interaction
    User([User]) -->|Query| ToolsTable[agents.tools]
    User -->|Selective Memory| MemoryBankTable[agents.memory_bank]
    User -->|Upload Files| SourceTables["agents.collection, agents.images, agents.videos, agents.audios"]
    User -->|Generate Image| ImageGenTable[agents.image_generation_tasks]

    %% Main Agent Workflow
    ToolsTable -->|Prompt| DocSearch[Search Documents]
    ToolsTable -->|Prompt| ImageSearch[Search Images]
    ToolsTable -->|Prompt| VideoFrameSearch[Search Video Frames]

    ToolsTable -->|Prompt, Tools| InitialLLM[Claude 3.5 - Tools]
    AvailableTools["**Available Tools**:
    get_latest_news
    fetch_financial_data
    search_news
    search_video_transcripts
    search_audio_transcripts"] -.-> InitialLLM
    InitialLLM -->|Tool Choice| ToolExecution[pxt.invoke_tools]
    ToolExecution --> ToolOutput[Tool Output]

    %% Context Assembly
    DocSearch -->|Context| AssembleTextContext[Assemble Text Context]
    ImageSearch -->|Context| AssembleFinalMessages[Assemble Final Messages]
    VideoFrameSearch -->|Context| AssembleFinalMessages

    ToolOutput -->|Context| AssembleTextContext
    AssembleTextContext -->|Text Summary| AssembleFinalMessages
    ToolsTable -->|Recent History| AssembleFinalMessages
    MemIndex -->|Context| AssembleTextContext
    ChatHistIndex -->|Context| AssembleTextContext

    %% Final LLM Call & Output
    AssembleFinalMessages -->|Messages| FinalLLM[Claude 3.5 - Answer]
    FinalLLM -->|Answer| ExtractAnswer[Extract Answer]
    ExtractAnswer -->|Answer| User
    ExtractAnswer -->|Answer| LogChat[agents.chat_history]
    ToolsTable -->|User Prompt| LogChat

    %% Follow-up Generation
    FinalLLM -->|Answer| FollowUpLLM[Mistral Small - Follow-up]
    FollowUpLLM -->|Suggestions| User

    %% Image Generation Workflow
    ImageGenTable -->|Prompt| OpenAI_Dalle[DALL-E 3]
    OpenAI_Dalle -->|Image Data| ImageGenTable
    ImageGenTable -->|Retrieve Image| User

    %% Supporting Structures
    SourceTables --> Views[**Materialized Views**
    Chunks, Frames, Sentences]
    Views --> Indexes[Embedding Indexes
    E5, CLIP]
    MemoryBankTable --> MemIndex[Search Memory]
    LogChat --> ChatHistIndex[Search Conversations]

    %% Styling
    classDef table fill:#E1C1E9,stroke:#333,stroke-width:1px
    classDef view fill:#C5CAE9,stroke:#333,stroke-width:1px
    classDef llm fill:#FFF9C4,stroke:#333,stroke-width:1px
    classDef workflow fill:#E1F5FE,stroke:#333,stroke-width:1px
    classDef search fill:#C8E6C9,stroke:#333,stroke-width:1px
    classDef tool fill:#FFCCBC,stroke:#333,stroke-width:1px
    classDef io fill:#fff,stroke:#000,stroke-width:2px

    class User io
    class ToolsTable,,SourceTables,ImageGenTable,LogChat,MemoryBankTable table
    class Views view
    class Indexes,MemIndex,ChatHistIndex search
    class InitialLLM,FinalLLM,FollowUpLLM,OpenAI_Dalle llm
    class DocSearch,ImageSearch,VideoFrameSearch,MemorySearch,ChatHistorySearch search
    class ToolExecution,AvailableTools,ToolOutput tool
    class AssembleTextContext,AssembleFinalMessages,ExtractAnswer workflow
```

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ .env                  # Environment variables (API keys, AUTH_MODE)
â”œâ”€â”€ .venv/                # Virtual environment files (if created here)
â”œâ”€â”€ data/                 # Default directory for uploaded/source media files
â”œâ”€â”€ logs/                 # Application logs
â”‚   â””â”€â”€ app.log
â”œâ”€â”€ static/               # Static assets for Flask frontend (CSS, JS, Images)
â”‚   â”œâ”€â”€ css/style.css
â”‚   â”œâ”€â”€ image/*.png
â”‚   â”œâ”€â”€ js/
â”‚   â”‚   â”œâ”€â”€ api.js
â”‚   â”‚   â””â”€â”€ ui.js
â”‚   â””â”€â”€ manifest.json
â”‚   â””â”€â”€ robots.txt
â”‚   â””â”€â”€ sitemap.xml
â”œâ”€â”€ templates/            # HTML templates for Flask frontend
â”‚   â””â”€â”€ index.html
â”œâ”€â”€ endpoint.py           # Flask backend: API endpoints and UI rendering
â”œâ”€â”€ functions.py          # Python UDFs and context assembly logic
â”œâ”€â”€ config.py             # Central configuration (model IDs, defaults, personas)
â”œâ”€â”€ requirements.txt      # Python dependencies
â””â”€â”€ setup_pixeltable.py   # Pixeltable schema definition script
```

## ğŸ“Š Pixeltable Schema Overview

Pixeltable organizes data in directories, tables, and views. This application uses the following structure within the `agents` directory:

```
agents/
â”œâ”€â”€ collection              # Table: Source documents (PDF, TXT, etc.)
â”‚   â”œâ”€â”€ document: pxt.Document
â”‚   â”œâ”€â”€ uuid: pxt.String
â”‚   â””â”€â”€ timestamp: pxt.Timestamp
â”œâ”€â”€ images                  # Table: Source images
â”‚   â”œâ”€â”€ image: pxt.Image
â”‚   â”œâ”€â”€ uuid: pxt.String
â”‚   â”œâ”€â”€ timestamp: pxt.Timestamp
â”‚   â””â”€â”€ thumbnail: pxt.String(computed) # Base64 sidebar thumbnail
â”œâ”€â”€ videos                  # Table: Source videos
â”‚   â”œâ”€â”€ video: pxt.Video
â”‚   â”œâ”€â”€ uuid: pxt.String
â”‚   â”œâ”€â”€ timestamp: pxt.Timestamp
â”‚   â””â”€â”€ audio: pxt.Audio(computed)      # Extracted audio (used by audio_chunks view)
â”œâ”€â”€ audios                  # Table: Source audio files (MP3, WAV)
â”‚   â”œâ”€â”€ audio: pxt.Audio
â”‚   â”œâ”€â”€ uuid: pxt.String
â”‚   â””â”€â”€ timestamp: pxt.Timestamp
â”œâ”€â”€ chat_history            # Table: Stores conversation turns
â”‚   â”œâ”€â”€ role: pxt.String        # 'user' or 'assistant'
â”‚   â”œâ”€â”€ content: pxt.String
â”‚   â””â”€â”€ timestamp: pxt.Timestamp
â”œâ”€â”€ memory_bank             # Table: Saved text/code snippets
â”‚   â”œâ”€â”€ content: pxt.String
â”‚   â”œâ”€â”€ type: pxt.String         # 'code' or 'text'
â”‚   â”œâ”€â”€ language: pxt.String    # e.g., 'python'
â”‚   â”œâ”€â”€ context_query: pxt.String # Original query or note
â”‚   â””â”€â”€ timestamp: pxt.Timestamp
â”œâ”€â”€ image_generation_tasks  # Table: Image generation requests & results
â”‚   â”œâ”€â”€ prompt: pxt.String
â”‚   â”œâ”€â”€ timestamp: pxt.Timestamp
â”‚   â””â”€â”€ generated_image: pxt.Image(computed) # DALL-E 3 output
â”œâ”€â”€ user_personas           # Table: User-defined personas
â”‚   â”œâ”€â”€ persona_name: pxt.String
â”‚   â”œâ”€â”€ initial_prompt: pxt.String
â”‚   â”œâ”€â”€ final_prompt: pxt.String
â”‚   â”œâ”€â”€ llm_params: pxt.Json
â”‚   â””â”€â”€ timestamp: pxt.Timestamp
â”œâ”€â”€ tools                   # Table: Main agent workflow orchestration
â”‚   â”œâ”€â”€ prompt: pxt.String
â”‚   â”œâ”€â”€ timestamp: pxt.Timestamp
â”‚   â”œâ”€â”€ user_id: pxt.String
â”‚   â”œâ”€â”€ initial_system_prompt: pxt.String
â”‚   â”œâ”€â”€ final_system_prompt: pxt.String
â”‚   â”œâ”€â”€ max_tokens, stop_sequences, temperature, top_k, top_p # LLM Params
â”‚   â”œâ”€â”€ initial_response: pxt.Json(computed)  # Claude tool choice output
â”‚   â”œâ”€â”€ tool_output: pxt.Json(computed)       # Output from executed tools (UDFs or Queries)
â”‚   â”œâ”€â”€ doc_context: pxt.Json(computed)       # Results from document search
â”‚   â”œâ”€â”€ image_context: pxt.Json(computed)     # Results from image search
â”‚   â”œâ”€â”€ video_frame_context: pxt.Json(computed) # Results from video frame search
â”‚   â”œâ”€â”€ memory_context: pxt.Json(computed)    # Results from memory bank search
â”‚   â”œâ”€â”€ chat_memory_context: pxt.Json(computed) # Results from chat history search
â”‚   â”œâ”€â”€ history_context: pxt.Json(computed)   # Recent chat turns
â”‚   â”œâ”€â”€ multimodal_context_summary: pxt.String(computed) # Assembled text context for final LLM
â”‚   â”œâ”€â”€ final_prompt_messages: pxt.Json(computed) # Fully assembled messages (incl. images/frames) for final LLM
â”‚   â”œâ”€â”€ final_response: pxt.Json(computed)    # Claude final answer generation output
â”‚   â”œâ”€â”€ answer: pxt.String(computed)          # Extracted text answer
â”‚   â”œâ”€â”€ follow_up_input_message: pxt.String(computed) # Formatted prompt for Mistral
â”‚   â”œâ”€â”€ follow_up_raw_response: pxt.Json(computed) # Raw Mistral response
â”‚   â””â”€â”€ follow_up_text: pxt.String(computed) # Extracted follow-up suggestions
â”œâ”€â”€ chunks                  # View: Document chunks via DocumentSplitter
â”‚   â””â”€â”€ (Implicit: EmbeddingIndex: E5-large-instruct on text)
â”œâ”€â”€ video_frames            # View: Video frames via FrameIterator (1 FPS)
â”‚   â””â”€â”€ (Implicit: EmbeddingIndex: CLIP on frame)
â”œâ”€â”€ video_audio_chunks      # View: Audio chunks from video table via AudioSplitter
â”‚   â””â”€â”€ transcription: pxt.Json(computed)   # Whisper transcription
â”œâ”€â”€ video_transcript_sentences # View: Sentences from video transcripts via StringSplitter
â”‚   â””â”€â”€ (Implicit: EmbeddingIndex: E5-large-instruct on text)
â”œâ”€â”€ audio_chunks            # View: Audio chunks from audio table via AudioSplitter
â”‚   â””â”€â”€ transcription: pxt.Json(computed)   # Whisper transcription
â””â”€â”€ audio_transcript_sentences # View: Sentences from direct audio transcripts via StringSplitter
    â””â”€â”€ (Implicit: EmbeddingIndex: E5-large-instruct on text)

# Available Tools (Registered via pxt.tools()):
# - functions.get_latest_news (UDF)
# - functions.fetch_financial_data (UDF)
# - functions.search_news (UDF)
# - search_video_transcripts (@pxt.query function)
# - search_audio_transcripts (@pxt.query function)

# Embedding Indexes Enabled On:
# - agents.chunks.text
# - agents.images.image
# - agents.video_frames.frame
# - agents.video_transcript_sentences.text
# - agents.audio_transcript_sentences.text
# - agents.memory_bank.content
# - agents.chat_history.content
```

## â–¶ï¸ Getting Started

### Prerequisites

You are welcome to swap any of the below calls, e.g. [WhisperX](https://docs.pixeltable.com/docs/examples/search/audio) instead of OpenAI Whisper, [Llama.cpp](https://docs.pixeltable.com/docs/integrations/frameworks#local-llm-runtimes) instead of Mistral... either through our built-in modules or by bringing your own models, frameworks, and API calls. See our [integration](https://docs.pixeltable.com/docs/integrations/frameworks) and [UDFs](https://docs.pixeltable.com/docs/datastore/custom-functions) pages to learn more. You can easily make this applicaiton entirely local if you decide to rely on local LLM runtimes and local embedding/transcription solutions.

-   Python 3.9+
-   API Keys:
    -   [Anthropic](https://console.anthropic.com/)
    -   [OpenAI](https://platform.openai.com/api-keys)
    -   [Mistral AI](https://console.mistral.ai/api-keys/)
    -   [NewsAPI](https://newsapi.org/) (100 requests per day free)

### Installation

```bash
# 1. Create and activate a virtual environment (recommended)
python -m venv .venv
# Windows: .venv\Scripts\activate
# macOS/Linux: source .venv/bin/activate

# 2. Install dependencies
pip install -r requirements.txt
```

### Environment Setup

Create a `.env` file in the project root and add your API keys. Keys marked with `*` are required for core LLM functionality.

```dotenv
# Required for Core LLM Functionality *
ANTHROPIC_API_KEY=sk-ant-api03-...  # For main reasoning/tool use (Claude 3.5 Sonnet)
OPENAI_API_KEY=sk-...             # For audio transcription (Whisper) & image generation (DALL-E 3)
MISTRAL_API_KEY=...               # For follow-up question suggestions (Mistral Small)

# Optional (Enable specific tools by providing keys)
NEWS_API_KEY=...                  # Enables the NewsAPI tool
# Note: yfinance and DuckDuckGo Search tools do not require API keys.

# --- !!**Authentication Mode (required to run locally)**!! ---
# Set to 'local' to bypass the WorkOS authentication used at agent.pixeltable.com and to leverage a default user.
# Leaving unset will result in errors
AUTH_MODE=local
```

### Running the Application

1.  **Initialize Pixeltable Schema:**
    This script creates the necessary Pixeltable directories, tables, views, and computed columns defined in `setup_pixeltable.py`. Run this *once* initially.

    *Why run this?* This defines the data structures and the declarative AI workflow within Pixeltable. It tells Pixeltable how to store, transform, index, and process your data automatically.

    ```bash
    python setup_pixeltable.py
    ```

2.  **Start the Web Server:**
    This runs the Flask application using the Waitress production server by default.

    ```bash
    python endpoint.py
    ```

    The application will be available at `http://localhost:5000`.

**Data Persistence Note:** Pixeltable stores all its data (file references, tables, views, indexes) locally, typically in a `.pixeltable` directory created within your project workspace. This means your uploaded files, generated images, chat history, and memory bank are persistent across application restarts.

## ğŸ–±ï¸ Usage Overview

The web interface provides several tabs:

-   **Chat Interface**: Main interaction area. Ask questions, switch between chat and image generation modes. View results, including context retrieved (images, video frames) and follow-up suggestions. Save responses to the Memory Bank.
-   **Agent Settings**: Configure the system prompts (initial for tool use, final for answer generation) and LLM parameters (temperature, max tokens, etc.) used by Claude.
-   **Chat History**: View past queries and responses. Search history and view detailed execution metadata for each query. Download history as JSON.
-   **Generated Images**: View images created using the image generation mode. Search by prompt, view details, download, or delete images.
-   **Memory Bank**: View, search, manually add, and delete saved text/code snippets. Download memory as JSON.
-   **How it Works**: Provides a technical overview of how Pixeltable powers the application's features.

## â­ Key Features

-   ğŸ’¾ **Unified Multimodal Data Management**: Ingests, manages, process, and index documents (text, PDFs, markdown), images (JPG, PNG), videos (MP4), and audio files (MP3, WAV) using Pixeltable's specialized [data types](https://docs.pixeltable.com/docs/datastore/bringing-data).
-   âš™ï¸ **Declarative AI Workloads**: Leverages Pixeltable's **[computed columns](https://docs.pixeltable.com/docs/datastore/computed-columns)** and **[views](https://docs.pixeltable.com/docs/datastore/views)** to declaratively define complex conditional workflows including data processing (chunking, frame extraction, audio extraction), embedding generation, AI model inference, and context assembly while maintaining data lineage and versioning.
-   ğŸ§  **Agentic RAG & Tool Use**: The agent dynamically decides which tools to use based on the query. Available tools include:
    -   **External APIs**: Fetching news (NewsAPI, DuckDuckGo), financial data (yfinance).
    -   **Internal Knowledge Search**: Pixeltable `@pxt.query` functions are registered as tools, allowing the agent to search video transcripts and audio transcripts on demand, as an example.
-   ğŸ” **Semantic Search**: Implements [vector search](https://docs.pixeltable.com/docs/datastore/vector-database) across multiple modalities, powered by any **embedding indexes** that Pixeltable incrementally and automatically maintain:
    -   Document Chunks (`sentence-transformers`)
    -   Images & Video Frames (`CLIP`)
    -   Chat History (`sentence-transformers`)
    -   Memory Bank items (`sentence-transformers`)
-   ğŸ”Œ **LLM Integration**: Seamlessly [integrates](https://docs.pixeltable.com/docs/integrations/frameworks) multiple LLMs for different tasks within the Pixeltable workflow:
    -   **Reasoning & Tool Use**: Anthropic Claude 3.5 Sonnet
    -   **Audio Transcription**: OpenAI Whisper (via computed columns on audio chunks)
    -   **Image Generation**: OpenAI DALL-E 3 (via computed columns on image prompts)
    -   **Follow-up Suggestions**: Mistral Small Latest
-   ğŸ’¬ **Chat History**: Persistently stores conversation turns in a Pixeltable [table](https://docs.pixeltable.com/docs/datastore/tables-and-operations) (`agents.chat_history`), enabling retrieval and semantic search over past interactions.
-   ğŸ“ **Memory Bank**: Allows saving and semantically searching important text snippets or code blocks stored in a dedicated Pixeltable table (`agents.memory_bank`).
-   ğŸ–¼ï¸ **Image Generation**: Generates images based on user prompts using DALL-E 3, orchestrated via a Pixeltable table (`agents.image_generation_tasks`).
-   ğŸ  **Local Mode**: Supports running locally without external authentication ([WorkOS](https://github.com/workos/python-flask-example-applications)) (`AUTH_MODE=local`) for easier setup and development.
-   ğŸ–¥ï¸ **Responsive UI**: A clean web interface built with Flask, Tailwind CSS, and JavaScript.
-   ğŸ› ï¸ **Centralized Configuration**: Uses an arbitraty `config.py` to manage model IDs, default system prompts, LLM parameters, and persona presets.

## âš ï¸ Disclaimer

This application serves as a comprehensive demonstration of Pixeltable's capabilities for managing complex multimodal AI workflows, covering data storage, transformation, indexing, retrieval, and serving.

The primary focus is on illustrating Pixeltable patterns and best practices within the `setup_pixeltable.py` script and related User-Defined Functions (`functions.py`).

While functional, less emphasis was placed on optimizing the Flask application (`endpoint.py`) and the associated frontend components (`style.css`, `index.html`, `ui.js`...). These parts should not necessarily be considered exemplars of web development best practices.

For simpler examples demonstrating Pixeltable integration with various frameworks (FastAPI, React, TypeScript, Gradio, etc.), please refer to the [Pixeltable Examples Documentation](https://docs.pixeltable.com/docs/examples/use-cases).
